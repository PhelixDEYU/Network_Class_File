{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEcSkzOVz_E_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install diffusers\n",
        "!pip install accelerate\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Gv2utunn1BZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"distil-whisper/distil-large-v3\")"
      ],
      "metadata": {
        "id": "aGXHz8ER1l10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"distil-whisper/distil-large-v3\")\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"distil-whisper/distil-large-v3\")"
      ],
      "metadata": {
        "id": "FfIQcxma1161"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade transformers accelerate datasets[audio]"
      ],
      "metadata": {
        "id": "lboazwMT15Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"distil-whisper/distil-large-v3\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "sample = dataset[0][\"audio\"]\n",
        "\n",
        "result = pipe(sample)\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "SPcNBX9J2T8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KlBgp5v-258q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# 定义Google Drive文件的URL\n",
        "url = 'https://drive.google.com/file/d/1Zfm5HYCSA7Sz8jdq0zJQTrQ_khmRvlD0/view?usp=sharing'\n",
        "\n",
        "# 定义文件保存路径\n",
        "output = '/content/Alone-30sec.MP3'\n",
        "\n",
        "# 使用gdown下载文件\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "q7HZ11og5GDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "import io\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "# 授權\n",
        "auth.authenticate_user()\n",
        "\n",
        "# 建立 Drive 服務\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# 下載文件\n",
        "file_id = '1Zfm5HYCSA7Sz8jdq0zJQTrQ_khmRvlD0'\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "fh = io.BytesIO()\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"下載進度：{0}\".format(status.progress() * 100))\n",
        "\n",
        "# 將文件保存到本地\n",
        "with open('Alone-30sec.wav', 'wb') as f:\n",
        "    fh.seek(0)\n",
        "    f.write(fh.read())\n"
      ],
      "metadata": {
        "id": "CqP_P4_f300V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(\"Alone-30sec.wav\")"
      ],
      "metadata": {
        "id": "zdEAkMyY6fyL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(\"Alone-30sec.wav\", return_timestamps=True)\n",
        "print(result[\"chunks\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-DoZTNm6wxW",
        "outputId": "a873e64a-55cd-4189-adcf-af4636f7134e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'timestamp': (0.0, 6.14), 'text': ' If this night is not forever, at least we are together.'}, {'timestamp': (6.74, 8.66), 'text': \" I know I'm not alone.\"}, {'timestamp': (9.26, 11.24), 'text': \" I know I'm not alone.\"}, {'timestamp': (12.12, 16.04), 'text': \" Anywhere forever, apart, we're still together.\"}, {'timestamp': (16.64, 18.54), 'text': \" I know I'm not alone.\"}, {'timestamp': (19.08, 21.96), 'text': \" I know I'm not alone.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n"
      ],
      "metadata": {
        "id": "A0WRB3D-71T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
        ")"
      ],
      "metadata": {
        "id": "aXUTZfPg7vZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 將給定的文字轉換為列表\n",
        "prompts = [inputs[\"prompts\"]]\n",
        "\n",
        "# 使用更新後的inputs來生成文本\n",
        "outputs = hf.generate(prompts=prompts)\n",
        "\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvX4bPqs8HJ8",
        "outputId": "0a662510-dc20-461e-b9bb-8a64942ebf06"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text=\" If this night is not forever, at least we are together.  I know I'm not alone.  I know I'm not alone.  Anywhere forever, apart, we're still together.  I know I'm not alone.  I know I'm not alone.   All of that goes on. \\n\")]] llm_output=None run=[RunInfo(run_id=UUID('f40efea6-f6c6-4391-a3a4-021ab6bf8288'))]\n"
          ]
        }
      ]
    }
  ]
}